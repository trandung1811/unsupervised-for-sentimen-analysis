{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_lowercase(text): \n",
    "    \n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text): \n",
    "    result = re.sub(r'\\d+', '', text) \n",
    "    \n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text): \n",
    "    \n",
    "    translator = str.maketrans('', '', string.punctuation) \n",
    "    \n",
    "    return text.translate(translator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(text): \n",
    "    \n",
    "    return  \" \".join(text.split()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def lemmatize_word(text): \n",
    "    \n",
    "    word_tokens = word_tokenize(text) \n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens] \n",
    "    \n",
    "    return lemmas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'data.txt'\n",
    "filename = 'restaurant_Reviews.tsv'\n",
    "def read_data(filename):\n",
    "    \n",
    "    f = open(filename,'r')\n",
    "    \n",
    "    text_list = f.read().split('\\n')\n",
    "    class_ = {}\n",
    "    \n",
    "    for i in range(1,len(text_list)):\n",
    "        class_[i] = text_list[i][-1]\n",
    "        text_list[i] = text_list[i].replace(text_list[i][-1],\"\")\n",
    "        \n",
    "    f.close()\n",
    "    \n",
    "    return text_list, class_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_data(text_list):\n",
    "    \n",
    "    sub_text = []\n",
    "    text = []\n",
    "    for i in range(1,len(text_list)):\n",
    "        sub_text = text_lowercase(remove_numbers(remove_punctuation(remove_whitespace(text_list[i]))))\n",
    "        text.append(lemmatize_word(sub_text))\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def main():\n",
    "    \n",
    "    sub_list, class_ = read_data(filename)\n",
    "    text_list = processing_data(sub_list)\n",
    "    \n",
    "    tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(text_list)]\n",
    "    model = Doc2Vec(tagged_data, vector_size=300, window=10, min_count=0, workers=4, epochs = 1000)\n",
    "    print(model.wv.most_similar('good'))\n",
    "    model.save('./imdb1.d2v')\n",
    "    f = open('clean_data.txt','w')\n",
    "    print(len(text_list))\n",
    "    for line in text_list:\n",
    "        for item in line:\n",
    "            f.write(str(item)+ \" \")\n",
    "        f.write('\\n')\n",
    "    f.close()\n",
    "    f= open('class.txt','w')\n",
    "    for item in class_:\n",
    "        f.write(\"%s\\n\" % class_[item])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \n",
    "    sub_list, class_ = read_data(filename)\n",
    "    text_list = processing_data(sub_list)\n",
    "    model = Word2Vec(min_count= 5, window=10, size=300, sample=1e-4, negative=20,alpha=0.03, \n",
    "                     min_alpha=0.0007, workers=multiprocessing.cpu_count()-1)\n",
    "    model.build_vocab(text_list,progress_per=50000)\n",
    "    model.train(text_list, total_examples=model.corpus_count, epochs=2000)\n",
    "    print(model.wv.most_similar('good'))\n",
    "    model.save('./imdb.d2v')\n",
    "    f = open('clean_data.txt','w')\n",
    "    print(len(text_list))\n",
    "    for line in text_list:\n",
    "        for item in line:\n",
    "            f.write(str(item)+ \" \")\n",
    "        f.write('\\n')\n",
    "    f.close()\n",
    "    f= open('class.txt','w')\n",
    "    for item in class_:\n",
    "        f.write(\"%s\\n\" % class_[item])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tasty', 0.22281697392463684), ('real', 0.21669557690620422), ('bread', 0.19579121470451355), ('sweet', 0.18715950846672058), ('selection', 0.178496316075325), ('its', 0.16531610488891602), ('because', 0.1620798110961914), ('sandwich', 0.14854556322097778), ('like', 0.14764533936977386), ('bad', 0.14491130411624908)]\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
